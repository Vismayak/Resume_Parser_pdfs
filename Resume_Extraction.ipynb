{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.omkarpathak.in/2018/12/18/writing-your-own-resume-parser/\n",
    "import PyPDF2\n",
    "import spacy\n",
    "import io\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import string as str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def text_extraction(file_path):\n",
    "    \n",
    "#   Step 1- Open the file\n",
    "    with open(file_path,'rb') as resume:\n",
    "#       Step 2- Read the pages of the pdfpages\n",
    "        for page in PDFPage.get_pages(resume,check_extractable=True,caching=True):\n",
    "#             Step 3- Create a ResouceManagager (ResourceManager facilitates reuse of shared resources \n",
    "#               such as fonts and images so that large objects are not allocated multiple times.)\n",
    "            resource_manager= PDFResourceManager()\n",
    "#             Step 4- Create a filehandle that would collect the extracted text\n",
    "            file_handle= io.StringIO()\n",
    "#             Step 5- Create a converter that uses the filehandle and the resource manager and needs to provided with the encoding\n",
    "            converter = TextConverter(resource_manager,file_handle,codec='utf-8',laparams=LAParams())\n",
    "#             Step 6- Create an interpreter that uses the converter and resource manager to extract text from pdf\n",
    "            interpreter = PDFPageInterpreter(resource_manager,converter)\n",
    "            interpreter.process_page(page)\n",
    "#             Step 7- Get text from file handle\n",
    "            text = file_handle.getvalue()\n",
    "            yield text\n",
    "            converter.close()\n",
    "            file_handle.close()\n",
    "text= ''\n",
    "# calling above function and extracting text\n",
    "file= \"Vismayak\\'s Resume.pdf\"\n",
    "for page in text_extraction(file):\n",
    "    text += ' ' + page\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vismayak Mohanarajan\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')    # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "matcher = Matcher(nlp.vocab, validate=True)          # Match sequences of tokens, based on pattern rules\n",
    "def get_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)       # Tag text accordingly\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]    #looks for a POS pattern of two proper nouns together \n",
    "    matcher.add('NAME', None, pattern)\n",
    "    matches = matcher(nlp_text)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "    \n",
    "name= get_name(resume_text=text)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Email ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mohanar2@illinois.edu\n"
     ]
    }
   ],
   "source": [
    "matcher.remove('NAME')\n",
    "def get_email(resume_text):\n",
    "    nlp_text = nlp(resume_text)       # Tag text accordingly\n",
    "    pattern = [{'LIKE_EMAIL': True}]    #looks for a POS pattern of two proper nouns together \n",
    "    matcher.add('EMAIL', None, pattern)\n",
    "    matches = matcher(nlp_text)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "email= get_email(text)\n",
    "print (email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217-974-5947\n"
     ]
    }
   ],
   "source": [
    "# matcher.remove('NUMBER')\n",
    "def get_num(resume_text):\n",
    "    nlp_text = nlp(resume_text) \n",
    "    phone_expression = r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'\n",
    "    for match in re.finditer(phone_expression, nlp_text.text):\n",
    "        start, end = match.span()\n",
    "        span = nlp_text.char_span(start, end) \n",
    "        return span.text\n",
    "number = get_num(text)\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Education Variables (Degrees, Subject, Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "majors = pd.read_csv(\"majors-list.csv\")\n",
    "majors = set(majors[\"Major\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Bachelor', 'Minor'], ['COMPUTER ENGINEERING', 'STATISTICS'], [])\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH','BACHELOR','MASTER','MINOR'\n",
    "        ]\n",
    "\n",
    "def get_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1] \n",
    "    \n",
    "    education_qual= list(edu.keys())\n",
    "    \n",
    "#     Extract Major\n",
    "    matches = {x for x in majors for j in edu.keys() if x in edu[j].upper() }\n",
    "    majors_or_minors = list(matches)\n",
    "    # Extract year\n",
    "    years = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            years.append(year[0])\n",
    "        \n",
    "    return education_qual,majors_or_minors,years\n",
    "\n",
    "print(extract_education(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skill Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # reading the csv file I created\n",
    "    data = pd.read_csv(\"skills.csv\") \n",
    "    \n",
    "    skills = []\n",
    "    skills =[i for x in data.columns for i in data[x] ]\n",
    "    skills = list(skills)\n",
    "    \n",
    "#     The one from github \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    skillset = {}\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            if token.lower() in skillset:\n",
    "                skillset[token.lower()] += 1\n",
    "            else:\n",
    "                skillset[token.lower()] = 1\n",
    "            \n",
    "    \n",
    "    # check |for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            if token in skillset:\n",
    "                skillset[token] += 1\n",
    "            else:\n",
    "                skillset[token] = 1\n",
    "    \n",
    "    return skillset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_skills(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
