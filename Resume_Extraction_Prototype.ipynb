{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": "# https://www.omkarpathak.in/2018/12/18/writing-your-own-resume-parser/\nimport PyPDF2\nimport spacy\nimport io\nfrom spacy.matcher import Matcher\nimport re\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport string as str"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Text Extraction from PDF"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": "from pdfminer.converter import TextConverter\nfrom pdfminer.pdfinterp import PDFPageInterpreter\nfrom pdfminer.pdfinterp import PDFResourceManager\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfpage import PDFPage\n\ndef text_extraction(file_path):\n    \n#   Step 1- Open the file\n    with open(file_path,'rb') as resume:\n#       Step 2- Read the pages of the pdfpages\n        for page in PDFPage.get_pages(resume,check_extractable=True,caching=True):\n#             Step 3- Create a ResouceManagager (ResourceManager facilitates reuse of shared resources \n#               such as fonts and images so that large objects are not allocated multiple times.)\n            resource_manager= PDFResourceManager()\n#             Step 4- Create a filehandle that would collect the extracted text\n            file_handle= io.StringIO()\n#             Step 5- Create a converter that uses the filehandle and the resource manager and needs to provided with the encoding\n            converter = TextConverter(resource_manager,file_handle,codec='utf-8',laparams=LAParams())\n#             Step 6- Create an interpreter that uses the converter and resource manager to extract text from pdf\n            interpreter = PDFPageInterpreter(resource_manager,converter)\n            interpreter.process_page(page)\n#             Step 7- Get text from file handle\n            text = file_handle.getvalue()\n            yield text\n            converter.close()\n            file_handle.close()\ntext= ''\n# calling above function and extracting text\nfile= \"Vismayak\\'s Resume.pdf\"\nfor page in text_extraction(file):\n    text += ' ' + page\n# print(text)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Get Name"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Vismayak Mohanarajan\n"
    }
   ],
   "source": "# load pre-trained model\nnlp = spacy.load('en_core_web_sm')    # Load English tokenizer, tagger, parser, NER and word vectors\nmatcher = Matcher(nlp.vocab, validate=True)          # Match sequences of tokens, based on pattern rules\ndef get_name(resume_text):\n    nlp_text = nlp(resume_text)       # Tag text accordingly\n    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]    #looks for a POS pattern of two proper nouns together \n    matcher.add('NAME', None, pattern)\n    matches = matcher(nlp_text)\n    for match_id, start, end in matches:\n        span = nlp_text[start:end]\n        return span.text\n    \nname= get_name(resume_text=text)\nprint(name)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Get Email ID"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mohanar2@illinois.edu\n"
    }
   ],
   "source": "matcher.remove('NAME')\ndef get_email(resume_text):\n    nlp_text = nlp(resume_text)       # Tag text accordingly\n    pattern = [{'LIKE_EMAIL': True}]    #looks for a POS pattern of two proper nouns together \n    matcher.add('EMAIL', None, pattern)\n    matches = matcher(nlp_text)\n    for match_id, start, end in matches:\n        span = nlp_text[start:end]\n        return span.text\nemail= get_email(text)\nprint (email)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Get Number"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "217-974-5947\n"
    }
   ],
   "source": "# matcher.remove('NUMBER')\ndef get_num(resume_text):\n    nlp_text = nlp(resume_text) \n    phone_expression = r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'\n    for match in re.finditer(phone_expression, nlp_text.text):\n        start, end = match.span()\n        span = nlp_text.char_span(start, end) \n        return span.text\nnumber = get_num(text)\nprint(number)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Get Education Variables (Degrees, Subject, Year)"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": "majors = pd.read_csv(\"majors-list.csv\")\nmajors = set(majors[\"Major\"])"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(['Bachelor', 'Minor'], ['COMPUTER ENGINEERING', 'STATISTICS'], [])\n"
    }
   ],
   "source": "nlp = spacy.load('en_core_web_sm')\n\n# Grad all general stop words\nSTOPWORDS = set(stopwords.words('english'))\n\n# Education Degrees\nEDUCATION = [\n            'BE','B.E.', 'B.E', 'BS', 'B.S', \n            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n            'BTECH', 'B.TECH', 'M.TECH', 'MTECH','BACHELOR','MASTER','MINOR'\n        ]\n\ndef get_education(resume_text):\n    nlp_text = nlp(resume_text)\n\n    # Sentence Tokenizer\n    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n\n    edu = {}\n    # Extract education degree\n    for index, text in enumerate(nlp_text):\n        for tex in text.split():\n            # Replace all special symbols\n            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n                edu[tex] = text + nlp_text[index + 1] \n    \n    education_qual= list(edu.keys())\n    \n#     Extract Major\n    matches = {x for x in majors for j in edu.keys() if x in edu[j].upper() }\n    majors_or_minors = list(matches)\n    # Extract year\n    years = []\n    for key in edu.keys():\n        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n        if year:\n            years.append(year[0])\n        \n    return education_qual,majors_or_minors,years\n\nprint(extract_education(text))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Skill Extraction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "    # reading the csv file I created\n    data = pd.read_csv(\"graybar_skills.csv\") \n    \n    skills = []\n    skills =[i for x in data.columns for i in data[x] ]\n    skills = list(skills)\n    \n#     The one from github \n    \n    data = pd.read_csv(\"skills.csv\") \n\n    skills = set(skills + (list(data.columns.values)))\n#     print(skills)"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport spacy\n\n# load pre-trained model\nnlp = spacy.load('en_core_web_sm')\n\ndef get_skills(resume_text):\n    nlp_text = nlp(resume_text)\n    noun_chunks = nlp_text.noun_chunks\n    # removing stop words and implementing word tokenization\n    tokens = [token.text for token in nlp_text if not token.is_stop]\n    \n    skillset = {}\n    \n    # check for one-grams (example: python)\n    for token in tokens:\n        if token.lower() in skills:\n            if token.lower() in skillset:\n                skillset[token.lower()] += 1\n            else:\n                skillset[token.lower()] = 1\n            \n    \n    # check |for bi-grams and tri-grams (example: machine learning)\n    for token in noun_chunks:\n        token = token.text.lower().strip()\n        if token in skills:\n            if token in skillset:\n                skillset[token] += 1\n            else:\n                skillset[token] = 1\n    \n    return skillset"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "get_skills(text)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
